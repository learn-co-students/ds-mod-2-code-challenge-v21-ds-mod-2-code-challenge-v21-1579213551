{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Code Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your Mod 2 Code Challenge. You will be tested for your understanding of concepts and ability to solve problems that have been covered in class and in the curriculum.\n",
    "\n",
    "Use any libraries you want to solve the problems in the code challenge.\n",
    "\n",
    "_Read the instructions carefully_. You will be asked both to write code and respond to a few short answer questions.\n",
    "\n",
    "**Note on the short answer questions**: For the short answer questions please use your own words. The expectation is that you have not copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, you should do your best to communicate yourself clearly.\n",
    "\n",
    "The sections of the code challenge are:\n",
    "- Statistical Distributions\n",
    "- Statistical Tests\n",
    "- Bayesian Statistics\n",
    "- Linear Regression and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "# import the necessary libraries\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Statistical Distributions [Suggested time: 25 minutes]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Normal Distributions\n",
    "\n",
    "Say we have check totals for all checks ever written at a TexMex restaurant. \n",
    "\n",
    "The distribution for this population of check totals happens to be normally distributed with a population mean of $\\mu = 20$ and population standard deviation of $\\sigma = 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a.1) Write a function to compute the z-scores for single checks of amount `check_amt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(check_amt):\n",
    "    \"\"\"\n",
    "    check_amt = the amount for which we want to compute the z-score\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "def z_score(check_amt):\n",
    "    \n",
    "    \"\"\" Using the formula (X - mu)/std \"\"\"\n",
    "    \n",
    "    return (check_amt - 20)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a.2) I go to the TexMex restaurant and get a check for 24 dollars. \n",
    "\n",
    "Use your function to compute your check's z-score, and interpret the result using the empirical rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "print(\"My check has a z-score of {}.\".format(z_score(24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# The z-score of your check tells you how many standard deviations your check amount is away from the mean\n",
    "# check amount. In this case, your check is two standard deviations away from the mean.\n",
    "\n",
    "# According to the empirical rule, 95% of expected check amounts will be within two standard deviations from the mean.\n",
    "# Your check amount is just within this region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a.3) Using $\\alpha = 0.05$, is my 25 dollar check significantly **greater** than the mean? How do you know this?  \n",
    "\n",
    "Hint: Here's a link to a [z-table](https://www.math.arizona.edu/~rsims/ma464/standardnormaltable.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "print(\"My check has a z-score of {}.\".format(z_score(24)))\n",
    "print(\"The critical threshold z is {}.\".format(round(stats.norm.ppf(0.95),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# For alpha = 0.05, the critical threshold z for an upper-tailed z-test is 1.64 (this can be found using the linked z-table or scipy.stats.)\n",
    "# We obtain a z-score of 2.5, which is greater than the critical threshold of 1.64. \n",
    "# Thus, my 25 dollar check is significantly greater than the mean at alpha = 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Confidence Intervals and the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b.1) Determine the 95% confidence interval around the mean check total for this population. Interpret your result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "# 95% confidence interval has z-score of 1.96 (read where p = 0.975)\n",
    "mean = 20\n",
    "std = 2\n",
    "conf = (mean - 1.96*std, mean + 1.96*std)\n",
    "print(\"The 95% confidence interval is \", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your written answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "\"\"\"\n",
    "A 95% confidence interval means that there is a 95% chance for the interval to contain the true population mean.\n",
    "\n",
    "A confidence interval is an interval containing the true population mean with a certain probability. \n",
    "i.e. For a 95% confidence interval, there is a 95% chance the interval contains the true population mean.\n",
    "\n",
    "Frequentist interpretation of the confidence interval:  \n",
    "Were this procedure to be repeated on 100 samples, approximately 95 of the calculated confidence intervals \n",
    "(which would be different for each sample) would be expected to contain the true population mean.\n",
    "\n",
    "INCORRECT: a confidence interval contains 95% of all values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b.2) Imagine that we didn't know how the population of check totals was distributed. How would **sampling** and the **Central Limit Theorem** allow us to **make inferences on the population mean**, i.e. estimate $\\mu, \\sigma$ of the population mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your written answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "\"\"\"\n",
    "Solution: The Central Limit Theorem says that we can take repeated samples of the population, \n",
    "and estimate population parameters by finding the average mean and standard deviation of the samples. \n",
    "Sample means will also tend to a normal distribution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Statistical Testing [Suggested time: 15 minutes]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TexMex restaurant recently introduced Queso to its menu.\n",
    "\n",
    "We have random samples of 1000 \"No Queso\" order check totals and 1000 \"Queso\" order check totals for orders made by different customers.\n",
    "\n",
    "In the cell below, we load the sample data for you into the arrays `no_queso` and `queso` for the \"no queso\" and \"queso\" order check totals. Then, we create histograms of the distribution of the check amounts for the \"no queso\" and \"queso\" samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample data \n",
    "no_queso = pickle.load(open(\"data/no_queso.pkl\", \"rb\"))\n",
    "queso = pickle.load(open(\"data/queso.pkl\", \"rb\"))\n",
    "\n",
    "# Plot histograms\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.set_title('Sample of Non-Queso Check Totals')\n",
    "ax1.set_xlabel('Amount')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.hist(no_queso, bins=20)\n",
    "\n",
    "ax2.set_title('Sample of Queso Check Totals')\n",
    "ax2.set_xlabel('Amount')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.hist(queso, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "# Load the sample data \n",
    "no_queso = pickle.load(open(\"data/no_queso.pkl\", \"rb\"))\n",
    "queso = pickle.load(open(\"data/queso.pkl\", \"rb\"))\n",
    "\n",
    "# Plot histograms\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.set_title('Sample of Non-Queso Check Totals')\n",
    "ax1.set_xlabel('Amount')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.hist(no_queso, bins=20)\n",
    "\n",
    "ax2.set_title('Sample of Queso Check Totals')\n",
    "ax2.set_xlabel('Amount')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.hist(queso, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Hypotheses and Errors\n",
    "\n",
    "The restaurant owners want to know if customers who order Queso spend **more or less** than customers who do not order Queso.\n",
    "\n",
    "2.a.1) Set up the null $H_{0}$ and alternative hypotheses $H_{A}$ for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your written answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "\n",
    "\"\"\"\n",
    "Null hypothesis: Customers who order queso spend the same as those who do not order queso. \n",
    "\n",
    "Alternative hypothesis: Customers who order queso do not spend the same as those who do not order queso. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a.2) What does it mean to make `Type I` and `Type II` errors in this specific context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "\"\"\"\n",
    "Type I: (Rejecting the null hypothesis given it's true): Saying queso customers' total check amounts are different \n",
    "than non-queso customers' total check amounts when they are the same.\n",
    "\n",
    "Type II: (Failing to reject the null hypothesis given it's false): Saying queso customers' total check amounts are \n",
    "the same as non-queso customers' total check amounts when they are different.\n",
    "\"\"\"\n",
    "\n",
    "# Give partial credit to students who describe what type I and type II errors are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Sample Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b.1) Run a statistical test on the two samples. Use a significance level of $\\alpha = 0.05$. You can assume the two samples have equal variance. Can you reject the null hypothesis? \n",
    "\n",
    "_Hint: Use `scipy.stats`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "\n",
    "# Run a two-tailed t-test\n",
    "print(stats.ttest_ind(no_queso, queso))\n",
    "\n",
    "# Students may compute the critical t-statistics for the rejection region\n",
    "critical_t = (stats.t.ppf(0.025, df=999), stats.t.ppf(0.975, df=999))\n",
    "print(critical_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# We have enough evidence to reject the null hypothesis at a significance level of alpha = 0.05. We obtain a p-value\n",
    "# much smaller than 0.025 (two-tailed test). Alternatively, our t-statistic is smaller than the critical t-statistic.\n",
    "# Both answers (p-value or critical t-statistic) are valid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Bayesian Statistics [Suggested time: 15 minutes]\n",
    "---\n",
    "### a. Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thomas wants to get a new puppy 🐕 🐶 🐩 \n",
    "\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/rD8R00QOKwfxC/giphy.gif\" />\n",
    "\n",
    "He can choose to get his new puppy either from the pet store or the pound. The probability of him going to the pet store is $0.2$. \n",
    "\n",
    "He can choose to get either a big, medium or small puppy.\n",
    "\n",
    "If he goes to the pet store, the probability of him getting a small puppy is $0.6$. The probability of him getting a medium puppy is $0.3$, and the probability of him getting a large puppy is $0.1$.\n",
    "\n",
    "If he goes to the pound, the probability of him getting a small puppy is $0.1$. The probability of him getting a medium puppy is $0.35$, and the probability of him getting a large puppy is $0.55$.\n",
    "\n",
    "3.a.1) What is the probability of Thomas getting a small puppy? \n",
    "\n",
    "3.a.2) Given that he got a large puppy, what is the probability that Thomas went to the pet store?\n",
    "\n",
    "3.a.3) Given that Thomas got a small puppy, is it more likely that he went to the pet store or to the pound?\n",
    "\n",
    "3.a.4) For Part 2, what is the prior, posterior and likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1 = None\n",
    "ans2 = None\n",
    "ans3 = \"answer here\"\n",
    "ans4_prior = \"answer here\"\n",
    "ans4_posterior = \"answer here\"\n",
    "ans4_likelihood = \"answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "ans1 = 0.2\n",
    "ans2 = 0.02/0.46\n",
    "ans3 = \"Pet Store\" # pet store! (0.12 vs 0.08)\n",
    "ans4_prior = \"P(Store)\"\n",
    "ans4_posterior = \"P(Store | Large)\"\n",
    "ans4_likelihood = \"P(Large | Store)\"\n",
    "\n",
    "\"\"\"\n",
    "Question 1:\n",
    "\n",
    "P(Small) = P(Small|Pet Store) + P(Small|Pound) = 0.2*0.6 + 0.8*0.1 = 0.2\n",
    "\n",
    "Question 2:\n",
    "\n",
    "P(Pet Store|Large)  = P(Large|Pet Store)*P(Pet Store) / P(Large) \n",
    "                    = 0.1*0.2 / (0.1*0.2 + 0.55*0.8)\n",
    "                    = 0.02 / 0.46 = 0.04348\n",
    "                    \n",
    "Question 3:\n",
    "\n",
    "P(Pet Store|Small) = 0.6\n",
    "P(Pound|Small) = 0.4\n",
    "\n",
    "More likely he went to the pet store.\n",
    "\n",
    "Question 4:\n",
    "P(Pet Store|Large) = P(Large|Pet Store)*P(Pet Store) / P(Large) \n",
    "\n",
    "Prior: P(Store)\n",
    "Posterior: P(Store | Large)\n",
    "Likelihood: P(Large | Store)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Linear Regression [Suggested Time: 10 min]\n",
    "---\n",
    "\n",
    "In this section, you'll be using the Advertising data, and you'll be creating linear models that are more complicated than a simple linear regression. The relevant modules have already been imported at the beginning of this notebook. We'll load and prepare the dataset for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/advertising.csv').drop('Unnamed: 0',axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "data = pd.read_csv('data/advertising.csv').drop('Unnamed: 0',axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing set. Do not change the random state please!\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# split the data into training and testing set. Do not change the random state please!\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Multiple Linear Regression\n",
    "\n",
    "In the linear regression section of the curriculum, you've analyzed how TV, Radio and Newspaper spendings individually affected the Sales figures. Here, we'll use all three together in a multiple linear regression model!\n",
    "\n",
    "4.a.1) Create a Correlation Matrix for `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.2) Based on this correlation matrix only, would you recommend to use `TV`, `radio` and `newspaper` in the same multiple linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your written answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# The highest correlation can be observed between radio and newspaper. \n",
    "# Since the correlation is only 0.35 (much smaller than what would be considered a high correlation ~>0.7), there\n",
    "# are no multicollinearity issues for the three variables, and from a multicollinearity perspective, \n",
    "# they can be used together in the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.3) Use StatsModels' `ols`-function to create a multiple linear regression model with `TV`, `radio` and `newspaper` as independent variables and sales as the dependent variable. Use the **training set only** to create the model.\n",
    "\n",
    "Required output: the model summary of this multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "train_data = pd.concat([X_train,y_train], axis = 1) # needed in the OLS-formula\n",
    "formula = 'y_train ~ X_train.TV + X_train.radio + X_train.newspaper'\n",
    "model = ols(formula = formula, data = train_data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.4) Do we have any statistically significant coefficients? If the answer is yes, list them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your written answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# Since the p-value is very small for TV and radio, they are statistically significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
