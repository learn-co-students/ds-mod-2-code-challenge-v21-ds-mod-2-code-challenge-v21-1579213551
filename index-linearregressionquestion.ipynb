{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Linear Regression and extensions [Suggested Time: 25 min]\n",
    "---\n",
    "\n",
    "In this section, you're going to be creating linear models that are more complicated than a simple linear regression. In the cells below, we are importing relevant modules that you might need later on. We also load and prepare the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       radio   newspaper       sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('raw_data/advertising.csv').drop('Unnamed: 0',axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing set. Do not change the random state please!\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Multiple Linear Regression\n",
    "\n",
    "In the linear regression section of the curriculum, you've analyzed how TV, Radio and Newspaper spendings individually affected the Sales figures. Here, we'll use all three together in a multiple linear regression model!\n",
    "\n",
    "4.a.1) Create a Correlation Matrix for `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TV</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054809</td>\n",
       "      <td>0.056648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radio</th>\n",
       "      <td>0.054809</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspaper</th>\n",
       "      <td>0.056648</td>\n",
       "      <td>0.354104</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TV     radio  newspaper\n",
       "TV         1.000000  0.054809   0.056648\n",
       "radio      0.054809  1.000000   0.354104\n",
       "newspaper  0.056648  0.354104   1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.2) Based on this correlation matrix only, would you recommend to use `TV`, `radio` and `newspaper` in the same multiple linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your written answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# The highest correlation can be observed between radio and newspaper. \n",
    "# Since the correlation is only 0.35 (much smaller than what would be considered a high correlation ~>0.7), there\n",
    "# are no multicollinearity issues for the three variables, and from a multicollinearity perspective, \n",
    "# they can be used together in the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.3) Use StatsModels' `ols`-function to create a multiple linear regression model with `TV`, `radio` and `newspaper` as independent variables and sales as the dependent variable. Use the **training set only** to create the model.\n",
    "\n",
    "Required output: the model summary of this multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y_train</td>     <th>  R-squared:         </th> <td>   0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   437.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 02 Jan 2020</td> <th>  Prob (F-statistic):</th> <td>9.93e-73</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:38:47</td>     <th>  Log-Likelihood:    </th> <td> -286.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   150</td>      <th>  AIC:               </th> <td>   580.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   146</td>      <th>  BIC:               </th> <td>   592.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>    2.9906</td> <td>    0.347</td> <td>    8.619</td> <td> 0.000</td> <td>    2.305</td> <td>    3.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train.TV</th>        <td>    0.0472</td> <td>    0.002</td> <td>   29.641</td> <td> 0.000</td> <td>    0.044</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train.radio</th>     <td>    0.1726</td> <td>    0.010</td> <td>   17.397</td> <td> 0.000</td> <td>    0.153</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train.newspaper</th> <td>    0.0031</td> <td>    0.007</td> <td>    0.471</td> <td> 0.639</td> <td>   -0.010</td> <td>    0.016</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>53.809</td> <th>  Durbin-Watson:     </th> <td>   2.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 145.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.447</td> <th>  Prob(JB):          </th> <td>2.43e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.862</td> <th>  Cond. No.          </th> <td>    432.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                y_train   R-squared:                       0.900\n",
       "Model:                            OLS   Adj. R-squared:                  0.898\n",
       "Method:                 Least Squares   F-statistic:                     437.5\n",
       "Date:                Thu, 02 Jan 2020   Prob (F-statistic):           9.93e-73\n",
       "Time:                        15:38:47   Log-Likelihood:                -286.41\n",
       "No. Observations:                 150   AIC:                             580.8\n",
       "Df Residuals:                     146   BIC:                             592.9\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept             2.9906      0.347      8.619      0.000       2.305       3.676\n",
       "X_train.TV            0.0472      0.002     29.641      0.000       0.044       0.050\n",
       "X_train.radio         0.1726      0.010     17.397      0.000       0.153       0.192\n",
       "X_train.newspaper     0.0031      0.007      0.471      0.639      -0.010       0.016\n",
       "==============================================================================\n",
       "Omnibus:                       53.809   Durbin-Watson:                   2.091\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              145.586\n",
       "Skew:                          -1.447   Prob(JB):                     2.43e-32\n",
       "Kurtosis:                       6.862   Cond. No.                         432.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__\n",
    "train_data = pd.concat([X_train,y_train], axis = 1) # needed in the OLS-formula\n",
    "formula = 'y_train ~ X_train.TV + X_train.radio + X_train.newspaper'\n",
    "model = ols(formula = formula, data = train_data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.4) Do we have any statistically significant coefficients? If the answer is yes, list them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the p-value is very small for TV and radio, they are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Polynomial terms\n",
    "\n",
    "We'd like to add a bit of complexity to the model created in the example above, and we will do it by adding some polynomial terms. Write a function to calculate train and test error for different polynomial degrees. You'll use `scikit-learn`'s `PolynomialFeatures`.\n",
    "\n",
    "This function should:\n",
    "* take `degree` as a parameter that will be used to create polynomial features to be used in a linear regression model\n",
    "* create a PolynomialFeatures object for each degree and fit a linear regression model using the transformed data\n",
    "* calculate the mean square error for each level of polynomial\n",
    "* return the `train_error` and `test_error` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(degree):\n",
    "    \"\"\"\n",
    "    Calculate train and test errorfor a linear regression with polynomial features.\n",
    "    (Hint: use PolynomialFeatures)\n",
    "    \n",
    "    input: Polynomial degree\n",
    "    output: Mean squared error for train and test set\n",
    "    \"\"\"\n",
    "    # // your code here //\n",
    "    \n",
    "    train_error = None\n",
    "    test_error = None\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "def polynomial_regression(degree):\n",
    "    \"\"\"\n",
    "    Calculate train and test errorfor a linear regression with polynomial features.\n",
    "    (Hint: use PolynomialFeatures)\n",
    "    \n",
    "    input: Polynomial degree\n",
    "    output: Mean squared error for train and test set\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree,interaction_only=False)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "    X_poly_test = poly.transform(X_test)\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_poly_train,y_train)\n",
    "    train_error = mean_squared_error(y_train, lr_poly.predict(X_poly_train))\n",
    "    test_error = mean_squared_error(y_test, lr_poly.predict(X_poly_test))\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out your new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24235967358392063, 0.15281375976869446)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24235967358392063, 0.15281375976869446)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__\n",
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your answers\n",
    "\n",
    "MSE for degree 3:\n",
    "- Train: 0.2423596735839209\n",
    "- Test: 0.15281375973923944\n",
    "\n",
    "MSE for degree 4:\n",
    "- Train: 0.18179109317368244\n",
    "- Test: 1.9522597174462015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. What is the optimal number of degrees for our polynomial features in this model? In general, how does increasing the polynomial degree relate to the Bias/Variance tradeoff?  (Note that this graph shows RMSE and not MSE.)\n",
    "\n",
    "<img src =\"visuals/rsme_poly_2.png\" width = \"600\">\n",
    "\n",
    "<!---\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "degree = list(range(1, 10 + 1))\n",
    "ax.plot(degree, error_train[0:len(degree)], \"-\", label=\"Train Error\")\n",
    "ax.plot(degree, error_test[0:len(degree)], \"-\", label=\"Test Error\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Polynomial Feature Degree\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Relationship Between Degree and Error\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"visuals/rsme_poly.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# The optimal number of features in this example is 3 because the testing error \n",
    "# is minimized at this point, and it increases dramatically with a higher degree polynomial. \n",
    "# As we increase the polynomial features, it is going to cause our training error to decrease, \n",
    "# which decreases the bias but increases the variance (the testing error increases). \n",
    "# In other words, the more complex the model, the higher the chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. In general what methods would you can use to reduce overfitting and underfitting? Provide an example for both and explain how each technique works to reduce the problems of underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "# Overfitting: Regularization. With regularization, more complex models are penalized. \n",
    "# This ensures that the models are not trained to too much \"noise.\"\n",
    "\n",
    "# Underfitting: Feature engineering. By adding additional features, you enable your \n",
    "# machine learning models to gain insights about your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
